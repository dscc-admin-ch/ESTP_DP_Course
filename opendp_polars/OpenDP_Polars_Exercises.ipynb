{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9410b1d4",
   "metadata": {},
   "source": [
    "# Introduction to Polars\n",
    "\n",
    "[Polars](https://docs.pola.rs/api/python/stable/reference/index.html) is a python library to manipulate and analyze tabular data through a high-performance DataFrame interface.\n",
    "\n",
    "It is very powerful for exploring, cleaning, and aggregating large datasets because it is built on top of Rust, runs operations in parallel across all CPU cores, and offers a lazy execution mode (more on that later) that optimizes your queries automatically.\n",
    "\n",
    "**Why Use Polars?**\n",
    "\n",
    "* High Performance – Built from the ground up in Rust, Polars runs close to the hardware and avoids heavy external dependencies.\n",
    "\n",
    "* Flexible I/O – Reads and writes common formats from local files, cloud storage, or databases with ease.\n",
    "\n",
    "* Intuitive API – You write clear, concise queries; Polars’ query optimizer figures out the fastest execution plan behind the scenes.\n",
    "\n",
    "* Out-of-Core Processing – The streaming API lets you work with datasets without requiring your data to be in memory at the same time\n",
    "\n",
    "* Automatic Parallelism – Uses all available CPU cores by default—no extra configuration required.\n",
    "\n",
    "* Optional GPU Acceleration – Can offload in-memory workloads to NVIDIA GPUs for even more speed.\n",
    "\n",
    "* Arrow Compatibility – Consumes and produces Apache Arrow data, often with zero-copy transfers, while maintaining its own optimized compute layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b4a47",
   "metadata": {},
   "source": [
    "The following benchmark, [conducted by H2O](https://h2oai.github.io/db-benchmark/), compares the speed of the leading data manipulation frameworks for performing group aggregation with a 50GB dataset:\n",
    "\n",
    "![benchmark](https://ssphub.netlify.app/post/polars/polars-benchmark-short.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cee5464",
   "metadata": {},
   "source": [
    "## Install the Library\n",
    "Polars is available on PyPi, it can easily be installed via the pip command. We will use the version compatible with opendp (1.32.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install polars==1.32.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a4827",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "In Polars, the core data structure is the DataFrame—a two-dimensional table of rows and columns.\n",
    "Each column is a Series, a one-dimensional labeled array. You can create DataFrames from Python dictionaries, lists, NumPy arrays, or pandas DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d3aa7",
   "metadata": {},
   "source": [
    "### Creating a polars DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the necessary libraries\n",
    "import polars as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9d643",
   "metadata": {},
   "source": [
    "Here is an example creating a DataFrame with information about cheeses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 10000\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "\n",
    "# Create a dictionary with sample data\n",
    "cheese_data = {\n",
    "    \"weight\": rng.uniform(100, 1000, size=num_rows),\n",
    "    \"cheese_year\": rng.integers(2019, 2025, size=num_rows),\n",
    "    \"canton\": rng.choice([\"Neuchatel\", \"Zurich\", \"Fribourg\", \"Valais\"], size=num_rows),\n",
    "    \"cheese_type\": rng.choice([\"Gruyere\", \"Vacherin\", \"Emmental\"], size=num_rows)\n",
    "}\n",
    "\n",
    "# Convert to a Polars DataFrame\n",
    "cheeses = pl.DataFrame(cheese_data)\n",
    "\n",
    "# Show the first few rows\n",
    "cheeses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364c80a",
   "metadata": {},
   "source": [
    "When you display **cheeses** in the console, a nice string representation of the DataFrame is displayed. \n",
    "The string representation first prints the shape of the data as a tuple with the first entry telling you the number of rows and the second the number of columns in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6253dc23",
   "metadata": {},
   "source": [
    "Similarly to library like `pandas`, you can save your dataframe in a csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese_path = \"./cheese.csv\"\n",
    "cheeses.write_csv(cheese_path)\n",
    "\n",
    "# Note: similarly, you could also read a csv with pl.read_csv(cheese_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c936eda9",
   "metadata": {},
   "source": [
    "Polars DataFrames are equipped with many useful methods and attributes for exploring the underlying data. \n",
    "\n",
    "If you’re already familiar with pandas, then you’ll notice that Polars DataFrames use mostly the same naming conventions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide an overview of the 5 first rows of the dataframe\n",
    "cheeses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c725ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide summary statistics\n",
    "cheeses.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides dictionnaries of the data type of each column \n",
    "cheeses.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc653777",
   "metadata": {},
   "source": [
    "## Contexts and expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e788c7",
   "metadata": {},
   "source": [
    "\n",
    "In Polars, **contexts** and **expressions** form the backbone of its data transformation syntax.\n",
    "\n",
    "- **Expressions** are operations or transformations applied to columns.  \n",
    "  These can include:\n",
    "  - Mathematical operations\n",
    "  - Aggregations\n",
    "  - Comparisons\n",
    "  - String manipulations\n",
    "  - And more...\n",
    "\n",
    "- **Contexts** define *where and how* these expressions are evaluated.  \n",
    "  Think of contexts as the \"action\" you want to perform on your data.\n",
    "\n",
    "Polars provides three main contexts:\n",
    "\n",
    "1. **Selection** → choosing columns  \n",
    "2. **Filtering** → extracting rows that meet conditions  \n",
    "3. **Groupby/Aggregation** → computing summary statistics within groups  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792d5e3",
   "metadata": {},
   "source": [
    "### 1. Selection\n",
    "\n",
    "In Polars, if you are interested in selecting just **one column**, it’s very straightforward.  \n",
    "\n",
    "You can use the `.select()` method with the column name wrapped in `pl.col()`. Or directly use `select()` method on the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cheeses.select(\"cheese_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c26cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cheeses.select(pl.col(\"cheese_type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc61d5",
   "metadata": {},
   "source": [
    "The second context, ``cheeses.select(pl.col(\"cheese_type\"))``, accomplishes the same task in a more powerful way because you can perform further manipulations on the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the column weight and sorting it in ascending order\n",
    "cheeses.select(pl.col(\"weight\").sort())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: select the cheese_year column and only display the last two digits (2019 ==> 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c5125",
   "metadata": {},
   "source": [
    "Note:  \n",
    "- `.select()` always returns a **DataFrame**, even if you only choose a single column.  \n",
    "- If you want the column as a **Series**, you can use `.get_column()` or dictionary-style access instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83fcd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(cheeses.select(\"cheese_type\")))\n",
    "print(type(cheeses.get_column(\"cheese_type\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dba52a9",
   "metadata": {},
   "source": [
    "### 2. Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccbb475",
   "metadata": {},
   "source": [
    "The `.filter()` method is used to **keep only the rows that match a condition**.  \n",
    "The condition itself is expressed using `pl.col()` together with comparison or logical operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4036f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "canton_ne = cheeses.filter(pl.col(\"canton\") == \"Neuchatel\")\n",
    "canton_ne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564d15a",
   "metadata": {},
   "source": [
    "Note:\n",
    "- `.filter()` always expects a **Boolean expression** (something that evaluates to True/False for each row).  \n",
    "- You can combine multiple conditions using logical operators:  \n",
    "  - `&` → AND  \n",
    "  - `|` → OR  \n",
    "  - `~` → NOT  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26acf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: filter to keep only cheeses that are bigger than 700g (weight) before 2022\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380d540",
   "metadata": {},
   "source": [
    "### 3. Group by / aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb39b0",
   "metadata": {},
   "source": [
    "The **groupby** (or **aggregation**) context is one of the most powerful tools in Polars.  \n",
    "It lets you split your dataset into subgroups and then compute **summary statistics** for each group.\n",
    "\n",
    "In our cheeses dataset, suppose we want to know:  \n",
    "- the **average weight** of cheeses,  \n",
    "- the **median weight** of cheeses, and  \n",
    "- the **total number of cheeses**\n",
    "\n",
    "for each Canton. Here’s how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b34d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cheeses.group_by(\"canton\").agg([\n",
    "    pl.col(\"weight\").mean().alias(\"avg_weight\"),\n",
    "    pl.col(\"weight\").median().alias(\"median_weight\"),\n",
    "    pl.len().alias(\"num_cheeses\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5fc755",
   "metadata": {},
   "source": [
    "Key points:\n",
    "- `.groupby(\"column\")` defines the subgroups you want to analyze.  \n",
    "- `.agg([...])` applies one or more expressions to compute summaries.  \n",
    "- `.alias()` helps rename the result columns for clarity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the same statistics but grouping per cheese_year and canton. Sort the result by median_weight by descending order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f67f2",
   "metadata": {},
   "source": [
    "## The Lazy API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0954495",
   "metadata": {},
   "source": [
    "So far, we’ve been working in **eager mode**:  \n",
    "- Every operation (`select`, `filter`, `groupby`, etc.) is executed immediately.  \n",
    "- The result is computed and returned right away.  \n",
    "- This means the **entire DataFrame is loaded into memory** before operations are applied.  \n",
    "\n",
    "Polars also provides a **Lazy API**, which changes how queries are executed. \n",
    "- Instead of loading everything in memory, it builds a **query plan**.  \n",
    "- Data is only scanned and materialized when you explicitly collect the result.  \n",
    "- This way, only the necessary columns and rows are read, making it more memory-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc41064",
   "metadata": {},
   "source": [
    "### What is the Lazy API?\n",
    "\n",
    "The Lazy API lets you **build a query plan first** (without running it immediately), and then **execute it later** with `.collect()`.\n",
    "\n",
    "This approach has several advantages:\n",
    "- **Query optimization**: Polars analyzes your entire query and rearranges or combines steps to make it faster.  \n",
    "- **Efficiency**: Only the data you actually need is processed.  \n",
    "- **Scalability**: Useful when working with large datasets that don’t fit in memory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08132b53",
   "metadata": {},
   "source": [
    "The core object within the Lazy API is the polars **LazyFrame**. Unlike traditional DataFrames, LazyFrames don’t contain data but instead store a set of instructions known as a query plan. Similarly to a DataFrame, you can create your LazyFrame directly from data (either by \"scanning\" an existing dataset or transforming an existing DataFrame into a lazy one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f68dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan a datasret lazily (efficient for large files for instance)\n",
    "# Only build a query plan, it does not load the file yet\n",
    "\n",
    "cheeses_lazy = pl.scan_csv(cheese_path)\n",
    "print(type(cheeses_lazy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b643f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or similary, we can build the lazyframe directly from the DataFrame (here cheeses)\n",
    "cheeses_lazy = pl.LazyFrame(cheeses)\n",
    "print(type(cheeses_lazy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10634cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that instead of showing the actual data, Polars prints out a logical plan\n",
    "\n",
    "cheeses_lazy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af978dfa",
   "metadata": {},
   "source": [
    "Once you are happy with your plan, you can use `collect()` to execute the plan on the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "cheeses_lazy.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b247f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "## Using the `cheeses` LazyFrame, build a query plan that:  \n",
    "##  1. Groups the data by Canton, and  \n",
    "##  2. Computes the total cheese weight for each Canton.  \n",
    "\n",
    "plan = \n",
    "\n",
    "plan.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59d3d3",
   "metadata": {},
   "source": [
    "When you run `.explain()` on a LazyFrame, Polars prints the **query plan**.  \n",
    "Let’s look at an example output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1985dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the plan\n",
    "\n",
    "print(plan.explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0579d0",
   "metadata": {},
   "source": [
    "How to read this?\n",
    "\n",
    "1. **AGGREGATE[maintain_order: false]**  \n",
    "   - Polars is doing a **groupby + aggregation**.  \n",
    "   - `maintain_order: false` means the groups may not appear in the original order (for performance reasons).  \n",
    "\n",
    "2. **[col(\"weight\").sum().alias(\"total_cheese_weight\")] BY [col(\"canton\")]**  \n",
    "   - This is the actual computation:  \n",
    "     - Take the `weight` column  \n",
    "     - Apply `.sum()`  \n",
    "     - Rename the result to `total_cheese_weight`  \n",
    "     - Group results by `canton`  \n",
    "\n",
    "3. **FROM DF [\"weight\", \"cheese_year\", \"canton\", \"cheese_type\"]**  \n",
    "   - The query starts from a DataFrame that has these 4 columns.  \n",
    "\n",
    "4. **PROJECT[\"weight\", \"canton\"] 2/4 COLUMNS**  \n",
    "   - Polars automatically dropped unused columns (`cheese_year`, `cheese_type`)  \n",
    "   - Only `weight` and `canton` are needed for the query, so it projects down to 2 out of 4 columns.  \n",
    "   - This is part of Polars’ **query optimization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f8eb4",
   "metadata": {},
   "source": [
    "# OpenDP Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97564d60",
   "metadata": {},
   "source": [
    "So far, we have explored how to use Polars for high-performance data analysis.  \n",
    "But what if we want to share insights from our data while protecting individual privacy?  \n",
    "\n",
    "This is where **[OpenDP](https://docs.opendp.org/en/stable/index.html)** comes in.  \n",
    "OpenDP is a library developed by Harvard that provides **differential privacy mechanisms**. Its goal is to make it easier for data scientists and researchers to build analyses that provide rigorous privacy guarantees.\n",
    "\n",
    "Key ideas in OpenDP:\n",
    "\n",
    "* **Core Concepts** – data domains, [metrics](https://docs.opendp.org/en/stable/api/python/opendp.metrics.html#module-opendp.metrics) for measuring changes in data, and [measures](https://docs.opendp.org/en/stable/api/python/opendp.measures.html#module-opendp.measures) for privacy guarantees.\n",
    "\n",
    "* **[Transformations](https://docs.opendp.org/en/stable/api/user-guide/transformations/index.html) and [Measurements](https://docs.opendp.org/en/stable/api/user-guide/measurements/index.html)** – deterministic transformations prepare data, while privacy mechanisms (e.g., Laplace or Gaussian noise) ensure differential privacy.\n",
    "\n",
    "    In OpenDP, a **Measurement** is the component that adds the noise required to satisfy differential privacy and produce a private release. A **Transformation**, by contrast, performs a purely deterministic mapping—used to preprocess or aggregate the data before it becomes private.\n",
    "\n",
    "* **Language Bindings** – a high-performance Rust core with Python and R interfaces, so you can write analyses in familiar languages.\n",
    "\n",
    "Some useful links: \n",
    "\n",
    "* https://docs.opendp.org/en/stable/api/user-guide/programming-framework/index.html\n",
    "\n",
    "---\n",
    "**The Polars integration**\n",
    "\n",
    "To simplify large-scale, tabular analysis, OpenDP now provides a Polars-based implementation by writing your standard Polars queries, then apply OpenDP’s privacy mechanisms directly to those queries.\n",
    "\n",
    "By combining **Polars** for data manipulation with **OpenDP** for privacy-preserving statistics, we can safely analyze and share sensitive data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796166f",
   "metadata": {},
   "source": [
    "\n",
    "## Install OpenDP\n",
    "We install the `opendp` library, which will allow us to build privacy-preserving analysis pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opendp==0.14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b83ccb5",
   "metadata": {},
   "source": [
    "## The `Context()` API\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c9871",
   "metadata": {},
   "source": [
    "OpenDP provides a high-level ``context()`` builder, but behind the scenes its core framework is deliberately low-level and modular. That flexibility lets advanced users assemble custom privacy transformations, but it also means you’d normally have to define **domains** (what kind of data you have), **metrics** (how you measure the distance between two datasets), and **measures** (how you track privacy loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ffc457",
   "metadata": {},
   "source": [
    "Because the core API exposes every building block—domains, metrics, measures, transformations, and measurements, it can feel heavy for someone who simply wants to run a private analysis.\n",
    "\n",
    "To make things easier, the library provides the Context API:\n",
    "\n",
    "* **Single entry point** – You create a Context object that holds your dataset (or a reference to it), privacy parameters (epsilon, delta), and configuration such as allowed query types.\n",
    "\n",
    "* **Query-driven** – Instead of chaining transformations and measurements manually, you issue high-level queries (counts, means, histograms, etc.).\n",
    "\n",
    "* **Automatic privacy accounting** – The context tracks how each query consumes the privacy budget and prevents you from exceeding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7faddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import opendp.prelude as dp\n",
    "\n",
    "# Enable experimental OpenDP features (in contrib) that are not part of the stable API.\n",
    "# Some of the polars features are still within the contrib, so we need to enable it\n",
    "dp.enable_features(\"contrib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d74acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LazyFrame for cheese\n",
    "cheeses_lf = pl.LazyFrame(cheeses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a278b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dp.Context.compositor(\n",
    "    data = cheeses_lf, # your lazy dataframe\n",
    "    privacy_unit = dp.unit_of(contributions=1), # each individual is allowed to contribute at most one row.\n",
    "    privacy_loss = dp.loss_of(epsilon = 1.0), # Sets the total privacy budget for this context to ε = 1.0\n",
    "    split_evenly_over=3, # Indicates you plan to run three separate queries.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362e2f4",
   "metadata": {},
   "source": [
    "Here, instead of directly manipulatig data with a polars LazyFrame, you now manipulate an `opendp.extras.polars.LazyFrameQuery` returned by `context.query()`.\n",
    "\n",
    "You can think of ``context.query()`` as a mock for the real data (although in reality, a ``LazyFrameQuery`` is an empty ``LazyFrame`` with some extra methods).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(context.query())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25780bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context.remaining_privacy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f3b84",
   "metadata": {},
   "source": [
    "We see that it has pre-allocated roughly a budget of 0.333 (**d_mids**) to each of the three expected queries. This helps us to keep track on the remaining budget and the number of queries we are left with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721effc",
   "metadata": {},
   "source": [
    "Then, to construct the query, OpenDP exends the Polars API to include differentially private methods and statistics. The `LazyFrameQuery` has additional methods such as `.summarize()` and `.release()` that can now be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910ce05",
   "metadata": {},
   "source": [
    "### `summarize()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c8455",
   "metadata": {},
   "source": [
    "**`summarize()`**: lets you inspect which mechanism and parameters will be used before spending privacy budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query to count the number of rows in the dataframe\n",
    "query = context.query().select(dp.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the statistics released by this query.\n",
    "query.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd56f35",
   "metadata": {},
   "source": [
    "We obtain a summary of the statistics that will be released  by this query, here is how we can read this table:\n",
    "*  **Column**: Logical name of the result column (\"len\" in this case).\n",
    "* **aggregate**: Human-readable description of the statistic being computed—here, “Frame Length,” i.e. row count.\n",
    "* **distribution**: The noise mechanism that will be applied to ensure differential privacy. \"Integer Laplace\" means Laplace noise adapted to integer outputs.\n",
    "* **scale**: The scale parameter of the Laplace mechanism. A larger value means more noise and stronger privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5f0dd",
   "metadata": {},
   "source": [
    "### ``release()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd2471",
   "metadata": {},
   "source": [
    "**`release()`**: With polars, once your plan is ready, we saw that we can use `collect()` to actually perform what we wanted on the actual data. With OpenDP, we first need to call `release()` before calling `collect()`. This accounts for the privacy loss of releasing the query and updates the privacy budget.\n",
    "\n",
    "If this step is not done, opendp will return you an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return you an error telling you to run release() before executing the query\n",
    "query.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af924322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What should be done\n",
    "query.release().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b55f8d",
   "metadata": {},
   "source": [
    "We know the actual DataFrame length is 10000, but after applying differential privacy the reported count is 9998, reflecting the added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context.remaining_privacy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b197e",
   "metadata": {},
   "source": [
    "The **d_mids** field shows that 0.33 ε of the total privacy budget has already been consumed by the first query.\n",
    "That leaves two queries remaining, each with roughly 0.33 ε available from the original allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b694532",
   "metadata": {},
   "source": [
    "## Aggregation with OpenDP Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f86336",
   "metadata": {},
   "source": [
    "Now that we have seen the basis. Let's see how we can use polars for more complex queries. You can find [here](https://docs.opendp.org/en/stable/getting-started/tabular-data/essential-statistics.html) the most common aggregators and how to use them in OpenDp (such as mean, median, sum, etc.).\n",
    "\n",
    "Let's start from a query plan we already performed with polars only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average weight and median weight of cheese in each Swiss Canton\n",
    "plan_polars = cheeses_lf.group_by(\"canton\").agg([\n",
    "    pl.col(\"weight\").mean().alias(\"avg_weight\"),\n",
    "    pl.col(\"weight\").median().alias(\"median_weight\"),\n",
    "    pl.len().alias(\"num_cheeses\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97033cb",
   "metadata": {},
   "source": [
    "**With Polars:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_polars.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c289e51",
   "metadata": {},
   "source": [
    "**With OpenDP:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691512fc",
   "metadata": {},
   "source": [
    "> 1. We setup the context\n",
    "\n",
    "*Note*: here, we need to specify the margin. This is used to describe what information is known publicly about a grouped dataset\n",
    "\n",
    "This is very useful to reduce the amount of noise added because certain aspects of the data are fixed or constrained by public knowledge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdf4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "context = dp.Context.compositor(\n",
    "    data = cheeses_lf,\n",
    "    privacy_unit = dp.unit_of(contributions=1),\n",
    "    privacy_loss = dp.loss_of(epsilon = 1.0),\n",
    "    split_evenly_over=1,\n",
    "    margins=[\n",
    "        dp.polars.Margin(\n",
    "            by=[\"canton\"],\n",
    "            invariant=\"keys\", # Assume the set of keys (categories) are public knowledge\n",
    "            max_length=10_000, # an upper bound on the number of records in any single group\n",
    "            max_groups= 4 # an upper bound on the number of distinct groups\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ffacc",
   "metadata": {},
   "source": [
    "> 2. Elaborate the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8956ddd2",
   "metadata": {},
   "source": [
    "2.1. Define your bounds / candidates\n",
    "\n",
    "You must always provide bounds (for continuous statistics like mean) or candidates (for discrete statistics like median) so OpenDP can safely compute sensitivity and add the correct amount of noise.\n",
    "\n",
    "Without this, the privacy guarantee would be invalid because the mechanism wouldn’t know the worst-case impact of a single individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = 1000\n",
    "lower_bound = 10\n",
    "candidates = list(range(lower_bound, upper_bound))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c933941",
   "metadata": {},
   "source": [
    "2.2. Write the query plan\n",
    "\n",
    "Note that we mostly use the same syntax as Polars, with a few twists directly related to differential privacy. \n",
    "\n",
    "To compute the mean, we use the method `dp.mean()`, allowing us to specify bounds. \n",
    "Bounds are necessary to correctly compute the sensitivity and calibrate the noise for privacy.\n",
    "\n",
    "For the median, we provide a finite set of possible values (candidates) for similar reasons.\n",
    "The mechanism must know the discrete universe of possible outputs to compute sensitivity.\n",
    "OpenDP will select a value from these candidates, adding noise to maintain privacy.\n",
    "\n",
    "Note: All these details are covered in the official OpenDP documentation, \n",
    "under \"Essential Statistics\".\n",
    "\n",
    "We also need to handle null/NaN values. Here, we simply replace them with 0, \n",
    "since the real dataset does not contain nulls.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf00fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_opendp = context.query().group_by(\"canton\").agg([\n",
    "    pl.col(\"weight\").fill_null(0).fill_nan(0).dp.mean(bounds=(lower_bound, upper_bound)).alias(\"avg_weight\"),\n",
    "    pl.col(\"weight\").fill_null(0).fill_nan(0).dp.median(candidates).alias(\"median_weight\"),\n",
    "    dp.len().alias(\"num_cheeses\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0b3a6",
   "metadata": {},
   "source": [
    "> 3. Release and collect the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d798216",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_opendp.release().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1afd3",
   "metadata": {},
   "source": [
    "**Small exercise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77904d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now, group the data by \"canton\" and compute the following for each group:\n",
    "# 1. The total (sum) of cheese weights.\n",
    "# 2. The 25th percentile (first quartile) of cheese weights.\n",
    "\n",
    "# Hints: First implement the query in plain Polars, then replicate it using OpenDP for differential privacy.\n",
    "\n",
    "# Polars\n",
    "plan_polars = ...\n",
    "\n",
    "\n",
    "print(plan_polars.collect())\n",
    "\n",
    "# OpenDp\n",
    "# You can use this context\n",
    "context = dp.Context.compositor(\n",
    "    data = cheeses_lf,\n",
    "    privacy_unit = dp.unit_of(contributions=1),\n",
    "    privacy_loss = dp.loss_of(epsilon = 1.0),\n",
    "    split_evenly_over=1,\n",
    "    margins=[\n",
    "        dp.polars.Margin(\n",
    "            by=[\"canton\"],\n",
    "            invariant=\"keys\",\n",
    "            max_length=10_000,\n",
    "            max_groups= 4\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# You'll need to use `dp.sum` and `dp.quantile`. \n",
    "# Details can be found here: https://docs.opendp.org/en/stable/getting-started/tabular-data/essential-statistics.html\n",
    "\n",
    "plan_opendp = ...\n",
    "\n",
    "print(plan_opendp.release().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb49fda",
   "metadata": {},
   "source": [
    "To go further: I highly recommend the [official documentation](https://docs.opendp.org/en/stable/api/user-guide/polars/expressions/index.html) of OpenDp on Expressions.\n",
    "\n",
    "The guide explains aggregations, boolean functions, and other transformations, helping you design privacy-preserving queries. There are also many practical example to show how to implement these tools in different contexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
